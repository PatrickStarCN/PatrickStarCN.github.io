<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[生成方法与判定方法]]></title>
    <url>%2F2018%2F04%2F25%2FGenApp-and-DisApp%2F</url>
    <content type="text"><![CDATA[监督学习方法分生成方法与判定方法，这篇文章是关于二者理解的笔记。 李航老师的《《统计学习方法》P18:生成方法由数据学习联合概率分布$P(X,Y)$,然后再求条件概率分布作为预测模型。典型的生成模型有：朴素贝叶斯，隐马尔科夫模型。判别方法有数据直接学习决策函数$f(X)$或者条件概率分布$P(XY)$作为预测模型。典型的判定模型有：KNN，决策树，LR，SVM等。 生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 生成方法对于历史数据有多少个类别就要生成多少个模型，然后用新数据特征去匹配这些模型，取最符合的。 判别方法根据历史数据直接生成一个判别式，新数据进来直接就可以判断他属于哪一个类别。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学基础之极大似然估计（MLE）]]></title>
    <url>%2F2018%2F04%2F25%2FMLE%2F</url>
    <content type="text"><![CDATA[“概率论只不过是把常识用数学公式表达了出来” —拉普拉斯 极大似然估计是机器学习算法常用的的一种参数估计方法，这篇文章是MLE和相关概念的笔记。 一些概念：先验概率，事情还没有发生，要求这件事情发生的可能性的大小，是根据以往经验和分析得到的概率。后验概率，事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。条件概率， 贝叶斯公式条件概率公式：$$P(A|B)=\frac{P(AB)}{P(B)}$$贝叶斯公式可以由条件概率公式推到出来：$$P(A_i|B)=\frac{P(A_iB)}{P(B)}=\frac{P(B|A_i)P(A_i)}{P(B)}$$贝叶斯公式中各部分含义： $P(A_i)$,$A_i$的先验概率 $P(B)$,$B$的先验概率 $P(A_i|B)$, 直观上理解，贝叶斯公式描述的是在$B$发生的情况下，$A_i$的概率 极大似然估计（MLE）]]></content>
      <categories>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>数学</tag>
        <tag>概率学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之LR(逻辑回归)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteLR%2F</url>
    <content type="text"><![CDATA[逻辑回归是一个回归函数，但是应用在分类问题上，可以对事件发生的概率进行预测。Todo Lost： 多元线性回归参数估计推导 为什么LR选择Sigmoid函数？ 梯度下降法的向量化？ 线性回归线性回归公式：$$f(x_i)=\omega x_i+b$$目的是使$f(x_i)$尽可能的靠近$y_i$(实际值). 求解$\omega$和$b$一般采均方误差最小化，这种方法又被称为“最小二乘法”，即找到一条直线使所有点到直线的欧氏距离最小，即最小化Cost Function: 最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form，而非线性最小二乘没有closed-form，通常用迭代法求解。 $$\begin{array}\left(\omega^*,b^*)&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(f(x_i)-u_i)^2}\\&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(y_i-\omega x_i-b)^2}\end{array}$$ 求解$\omega$和$b$，将上式分别对$\omega$和$b$求导有(即求偏导)： $$\omega =\cfrac{\sum\limits_{i=1}^m{y_i(x_i-\bar x)}}{\sum\limits_{i=1}^m{x_i^2-\cfrac1m\bigg(\sum\limits_{i=1}^m{x_i}\bigg)^2}}$$$$b=\cfrac1m\sum_{i=1}^m(y_i-\omega x_i)$$ 上述是一元线性回归的情况.对于多元线性回归则有： $$f(\boldsymbol x_i)=\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b$$或者写作$$ h_\theta(\boldsymbol x_i)=f(\theta^{\mathrm T}\boldsymbol x_i)=\cfrac1{1+\mathrm e^{-\theta^{\mathrm T}\boldsymbol x_i}}$$使$f(\boldsymbol x_i)$尽可能的靠近$y_i$(实际值） 多元线性回归参数待补完 Logistic Regression（对数几率回归，逻辑斯蒂回归）LR概念LR的思想是将线性回归输出的连续值$\left(-\infty,+\infty \right)$映射到（0,1）的一个概率值，从而能够根据阈值来进行分类。Sigmoid函数就正好满足需求，他的定义如下： Sigmoid函数是一个任意阶可导凸函数。$$y=\cfrac{1}{1+\mathrm{e}^{-z}}$$为什么选择Sigmoid函数？？？？？？ 则把线性回归的输出带入Sigmoid函数则有：$$\hat y=\cfrac{1}{1+\mathrm{e}^{-(\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b)}}$$ LR参数估计两种求解LR参数的方法，极大似然估计和梯度下降法。首先是LR的Cost function,如果使用和线性回归一样的损失函数会得到一个非凸函数，不利于后续的求解（比如用梯度下降法得到的就不一定是最优解）。LR的损失函数定义是($\hat y$为预测值，$y$为实际值)：$$\begin{array}\\J(\boldsymbol \omega,b)&amp;=&amp;\cfrac1m\sum_{i=1}^{m}L\left({\hat y_i}-y_i\right)\\&amp;=&amp;-\cfrac{1}{m}\sum_{i+1}^{m}[y_i\log(\hat y_i)+(1-y_i)log(1-\log(\hat y_i))]\end{array}$$ 关于LR的Cost Function我的理解:首先是合理性上，从公式看当$y^{(i)}=1$时，则损失函数可以简化为$J(\omega,b)=-\log({\hat y^{(i)}})$，则当$\hat y^{(i)}\to0$时，$J\to\infty$,即Cost趋近于无穷。当$\hat y^{(i)}\to1$时，$J\to 0$,即Cost趋近于0.$y^{(i)}=0$时，同理。所以这个Cost Function从概念上是符合我们的要求的。推导：将$y$视作是样本为正例的概率，则$1-y$是反例的概率,即：$$\begin{array}\\P(y=1|\boldsymbol x)=\hat y \\P(y=0|\boldsymbol x)=1-\hat y\end{array}$$结合上述二式则有：$$P(y|\boldsymbol x;\boldsymbol\omega,b)=\hat y^y(1-\hat y)^{(1-y)}$$有了条件概率公式我们则可以用极大似然估计求解参数，取$P(y|\boldsymbol x;\boldsymbol\omega,b)$在整个训练集上的似然函数有：$$L(\boldsymbol \omega,b)=\prod_{i=1}^{m}{\left[(\hat y_i)^{y_i}(1-\hat y_i)^{(1-y_i)}\right]}$$对似然函数取对数有：$$l(\boldsymbol\omega,b )=\mathrm{log}L(\boldsymbol \omega,b)=\sum_{i=1}^{m}{[y_i\mathrm{log}(\hat y _i)+(1-y_i)\mathrm{log}(1-\hat y_i)]}$$极大似然估计就是要求得使$l(\boldsymbol \omega,b)$最大时的$\boldsymbol\omega$和$b$的值，这里可以用极大似然估计本身的解法或者梯度上升法解决。而cost function值越小越好，所以Andrew Ng的课件里在$l(\boldsymbol \omega,b)$前乘了个系数$-\cfrac1m$当做最终的cost fucntion：$$J(\boldsymbol \omega,b)=-\cfrac1ml(\boldsymbol\omega,b )$$这样一来求使cost fucntion最小的$（\boldsymbol\omega,b）$就等于求解在给定样本情况下极大似然估计。 梯度下降法最最开始的时候没搞清楚梯度下降法和最小二乘法的区别，梯度下降法是求解非线性最小二乘法（没有闭式解）的方法之一。 为方便公式书写令$\boldsymbol\theta=(\boldsymbol\omega,b)$,则对$\boldsymbol\theta$的第$j$个属性按照梯度下降法更新则有： 这部分公式中的有的粗体用错了，之后有时间修正!!!!$$\theta_j=\theta_j-\alpha\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}$$具体梯度下降法过程推导如下：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac{\partial }{\theta_j}\cfrac1m\sum_{i=1}^m{[y_i\mathrm log(h_{\theta}(\boldsymbol x_i))+(1-y_i)\mathrm log(1-h_\theta(\boldsymbol x_i))]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}h_\theta(\boldsymbol x_i)-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}{h_\theta(\boldsymbol x_i)}\right]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)}\end{array}$$而$$\begin{array}\\\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)&amp;=&amp;\cfrac{\partial}{\partial\theta_j}\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\\&amp;=&amp;\cfrac{\mathrm e^{-\theta^\mathrm T\boldsymbol x_i}}{(1+\mathrm e^{-\theta^\mathrm T\boldsymbol x_i})^2}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}-1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\end{array}$$其中$x_{i,j}$表示第i个样本中的第j个属性,且$h_\theta(\boldsymbol x_i)=g(\theta^{\mathrm T}\boldsymbol x_i)$所以：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]}g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i(1-g(\theta^{\mathrm T}\boldsymbol x_i))-(1-y_i)g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i-g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp; \cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}\end{array}$$所以$\theta_j$的更新公式可以写作：$$\theta_j=\theta_j-\alpha\cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}$$ 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>LR</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之SVM(支持向量机)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteSVM%2F</url>
    <content type="text"><![CDATA[支持向量机是一种二分类模型，是特征空间上间隔最大化的线性分类器。这篇是关于SVM的笔记。 Todo list： 算法原理 算法推倒 算法优化 算法实现 一些疑问 基于训练集，SVM想找到一个最优划分超平面将不同的类别分开，学习的策略是使间隔最大化（之后说明），则该超平面可以写成：$$\boldsymbol\omega^\mathrm{T}\boldsymbol x+b=0$$其中$\boldsymbol\omega$为超平面法向量决定超平面方向，$b$为位移项决定超平面与原点的距离。令$f(\boldsymbol x)=\boldsymbol\omega^\mathrm{T}\boldsymbol x+b$则$f(\boldsymbol x)=0$时表示$\boldsymbol x$在超平面上。 样本数据线性可分的情况下，我们假设当$f(\boldsymbol x)&gt;0$时令其所对应的$y=+1$,当$f(\boldsymbol x)&lt;0$时令其所对应的$y=-1$ SVM为什么采用间隔最大化？当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。 几何间隔与函数间隔函数间隔的定义如下，对于某一样本点$\boldsymbol x_i$：$$\hat\gamma_i=yf(\boldsymbol x)=y(\boldsymbol\omega^{\mathrm{T}}\boldsymbol x+b)$$函数间隔保证自身的非负性,$f(\boldsymbol x)&lt;0$时$y=-1$。且$\hat\gamma_i$越大该样本点分类的可信度越高。但是当成比例改变$\boldsymbol\omega$和$b$时超平面不变，但函数间隔会变大。用几何间隔能更好的解决这个用问题，即样本点到超平面的距离：$$\gamma_i=\cfrac{|\boldsymbol\omega^{\mathrm{T}}\boldsymbol x+b|}{||\boldsymbol\omega||}=y_i\cfrac{\boldsymbol\omega^{\mathrm{T}}\boldsymbol x+b}{||\boldsymbol\omega||}=\cfrac{\hat\gamma_i}{||\boldsymbol\omega||}$$对于一整个样本集我们把最小的样本点的间隔当做整个样本的间隔即：$$\gamma=\min\limits_{i=1,\cdots,n} \hat\gamma_i$$ 函数间隔我觉得有点鸡肋，不是很明白其存在的意义，直接用几何上的直观解释，点到超平面距离即几何间隔就可以解决的问题。 任意点到超平面距离公式证明。设$\boldsymbol x_0$是样本点在超平面上的投影，即$\boldsymbol\omega^\mathrm{T}\boldsymbol x_0+b=0$，则$\overrightarrow{\boldsymbol x_0\boldsymbol x_i}$与法向量$\boldsymbol\omega$平行，所以：$$|\boldsymbol\omega\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=|\boldsymbol\omega||\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=\sqrt{\omega_1^2+\cdots+\omega_n^2}d=||\boldsymbol\omega||d$$又因为$\boldsymbol\omega^{\mathrm T}\boldsymbol x_0=-b$$$\begin{array}\boldsymbol\omega\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}&amp;=&amp;\omega^1(x_i^1-x_0^1)+\cdots+\omega^n(x_i^n-x_0^n)\\&amp;=&amp;\omega^1x_i^1+\cdots+\omega^nx_i^n-(\omega^1x_0^1+\dots+\omega^ix_0^i)\\&amp;=&amp;\boldsymbol\omega^{\mathrm T}\boldsymbol x_i+b\end{array}$$则有$$|\boldsymbol\omega^{\mathrm T}\boldsymbol x_i+b|=||\boldsymbol\omega||d\\d=\cfrac{|\boldsymbol\omega^{\mathrm T}\boldsymbol x_i+b|}{||\boldsymbol\omega||}$$ 最大化间隔假设使样本中离超平面最近的样本点与超平面保证一定距离（可以证明，这样的超平面只有一个），令最近的样本点的函数间隔为$y_i(\boldsymbol\omega^\mathrm{T}\boldsymbol x_i+b)$=1，这些最近的样本点被称为支持向量。则支持向量到超平面的几何间隔为：$$\gamma=\cfrac{1}{||\boldsymbol\omega||}$$所以为了最大化间隔，目标函数为：$$\begin{align}\max\limits_{\boldsymbol\omega,b}\cfrac{1}{||\boldsymbol\omega||}\\s.t. y_i&amp;(\boldsymbol\omega^\mathrm{T}\boldsymbol x_i+b)&gt;=1\end{align}$$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习性能评估指标]]></title>
    <url>%2F2018%2F04%2F23%2FThePerformanceEvaluationOfMachineLearning%2F</url>
    <content type="text"><![CDATA[机器学习在不同的业务需求下，需要使用不同的评估指标对算法效果进行评估，这篇文章将对常见的评估指标进行总结。 Todo list： 分类任务 Accuracy Precision Recall F1 Score Roc curve PR curve AUC 回归任务 MAE MSE 由于翻译问题，Accuracy（准确率），Precision(精确率，查准率)，Recall（召回率，查全率）中文表述会存在歧义，因此下文均用英文表述。 评估指标根据任务类型不同主要分为两类： 分类任务评估指标 Accuracy Precision Recall F1 Score Roc curve PR curve AUC 回归任务评价指标 MAE (Mean Absolute Error，绝对平均误差) MSE (Mean Squared Error，均方误差) 混淆矩阵提到评价方法首先我们先引入混淆矩阵（Confusion Matrix)的概念，因为接下来的分类任务的评价指标Accuracy，precision，recall，F1 score都可以用混淆举中的元素表示。 混淆矩阵是用来反映某一个分类模型的分类结果的，其中行代表的是真实的类，列代表的是模型的分类，如下表： 分类任务评价指标AccuracyAccuracy是评价指标里最易懂，最直观的一个。它的定义如下: $$ Accuracy= \frac{分类正确的样本数}{总样本数} $$ 用混淆矩阵表示就是：$$ Accuracy= \frac{TP+TN}{TP+TN+FP+FN} $$ 即Accuracy表示分类器正确分类的样本数与总样本数之比。 注意不均衡数据会对Accuracy产生极大影响。例如在做债务人逾期率预测任务时，逾期（即未在指定日期还款）的概率相对较低，在此我们假设100人里有一人逾期，如果分类器简单的把所有人都标记为非逾期（未进行预测），那么这个任务的Accuray将会是99%，99%的准确率看起来很高，然而并未反应分类器的好坏（分类器只是简单地标记所有样本为非逾期）。 Precision 和 Recall先从混淆矩阵定义来看Precision和Recall的定义：$$ Presion= \frac{TP}{TP+FP} $$$$ Recall= \frac{TP}{TP+FN} $$TP表示预测为正例中真正的正样本数，TP+FP表示预测为正的所有样本数，所以Precision表示的是预测为正例的样本中有多少是真正的正例。 同理TP+FN表示样本中正例的总数，所以Recall表示的是样本中的正例被准确预测的比例。 Precision和Recall有着自己的侧重点，在不同的业务需求下重要性也不同。下面举例说明： 推荐算法中，往往会给用户推荐较多的候补项，这时候我们希望我们推荐的候补项中尽可能多的是用户感兴趣的内容，这种情况我们希望Precision尽可能的大。 地震预测任务中，我们宁愿误报也不愿意错过一次可能的正确预测，所以我们的侧重点就是尽可能的提高样本中的正例被准确预测的比例，如果100次任务里只有一次地震，我们要提高这次地震被预测出来的概率，也就是“宁可错杀以前也不愿放走一个”，这种情况就希望Recall尽可能的大。 F1 ScoreF1 Score是一个综合考虑Precision和Recall的评价指标，他的定义是二者的调和平均：$$F1=\frac{1}{\frac{1}{2}(\frac{1}{Precision}+\frac{1}{Recall})}\implies F1=\frac{2\cdot Precision\cdot Recall}{Precision+Recall}$$Precision和Recall都高时F1 Score也会高。 F1 Score的变种$F_\beta=（1+\beta^2）\cdot\frac{Precision \cdot Recall}{\beta^2 \cdot Precision+Recall}$运行我们通过调节$\beta$值来调整Precision和Recall的权重。其中$\beta&gt;0$,当$\beta&gt;1$时Precision有更大影响，$\beta&lt;1$时Recall有更大影响，$\beta=1$时就是标准的F1。 PR Curve###]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>评估方法</tag>
      </tags>
  </entry>
</search>
