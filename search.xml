<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[生成方法与判定方法]]></title>
    <url>%2F2018%2F04%2F25%2FGenApp-and-DisApp%2F</url>
    <content type="text"><![CDATA[监督学习方法分生成方法与判定方法，这篇文章是关于二者理解的笔记。 李航老师的《《统计学习方法》P18:生成方法由数据学习联合概率分布$P(X,Y)$,然后再求条件概率分布作为预测模型。典型的生成模型有：朴素贝叶斯，隐马尔科夫模型。判别方法有数据直接学习决策函数$f(X)$或者条件概率分布$P(XY)$作为预测模型。典型的判定模型有：KNN，决策树，LR，SVM等。 生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 生成方法对于历史数据有多少个类别就要生成多少个模型，然后用新数据特征去匹配这些模型，取最符合的。 判别方法根据历史数据直接生成一个判别式，新数据进来直接就可以判断他属于哪一个类别。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之SVM(支持向量机)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteSVM(1)%2F</url>
    <content type="text"><![CDATA[支持向量机是一种二分类模型，是特征空间上间隔最大化的线性分类器。这篇是关于SVM的笔记。 Todo list： 算法原理 算法推倒 算法优化 算法实现 一些疑问 基于训练集，SVM想找到一个最优划分超平面将不同的类别分开，学习的策略是使间隔最大化（之后说明），则该超平面可以写成： $$\boldsymbol w^\mathrm{T}\boldsymbol x+b=0$$ 其中$\boldsymbol w$为超平面法向量决定超平面方向，$b$为位移项决定超平面与原点的距离。令$f(\boldsymbol x)=\boldsymbol w^\mathrm{T}\boldsymbol x+b$则$f(\boldsymbol x)=0$时表示$\boldsymbol x$在超平面上。 样本数据线性可分的情况下，我们假设当$f(\boldsymbol x)&gt;0$时令其所对应的$y=+1$,当$f(\boldsymbol x)&lt;0$时令其所对应的$y=-1$ SVM为什么采用间隔最大化？当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。 几何间隔与函数间隔函数间隔的定义如下，对于某一样本点$\boldsymbol x_i$：\begin{align}\hat\gamma_i=yf(\boldsymbol x)=y(\boldsymbol w^{\mathrm{T}}\boldsymbol x+b)\end{align}函数间隔保证自身的非负性,$f(\boldsymbol x)&lt;0$时$y=-1$。且$\hat\gamma_i$越大该样本点分类的可信度越高。但是当成比例改变$\boldsymbol w$和$b$时超平面不变，但函数间隔会变大。用几何间隔能更好的解决这个用问题，即样本点到超平面的距离：\begin{align}\gamma_i=\cfrac{|\boldsymbol w^{\mathrm{T}}\boldsymbol x+b|}{||\boldsymbol w||}=y_i\cfrac{\boldsymbol w^{\mathrm{T}}\boldsymbol x+b}{||\boldsymbol w||}=\cfrac{\hat\gamma_i}{||\boldsymbol w||}\end{align}对于一整个样本集我们把最小的样本点的间隔当做整个样本的间隔即：\begin{align}\gamma=\min\limits_{i=1,\cdots,n} \hat\gamma_i\end{align} 函数间隔我觉得有点鸡肋，不是很明白其存在的意义，直接用几何上的直观解释，点到超平面距离即几何间隔就可以解决的问题。 任意点到超平面距离公式证明。设$\boldsymbol x_0$是样本点在超平面上的投影，即$\boldsymbol w^\mathrm{T}\boldsymbol x_0+b=0$，则$\overrightarrow{\boldsymbol x_0\boldsymbol x_i}$与法向量$\boldsymbol w$平行，所以：\begin{align}|\boldsymbol w\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=|\boldsymbol w||\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=\sqrt{ w_1^2+\cdots+ w_n^2}d=||\boldsymbol w||d\end{align}又因为$\boldsymbol w^{\mathrm T}\boldsymbol x_0=-b$\begin{align}\boldsymbol w\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}&amp;= w^1(x_i^1-x_0^1)+\cdots+ w^n(x_i^n-x_0^n)\\&amp;= w^1x_i^1+\cdots+ w^nx_i^n-( w^1x_0^1+\dots+ w^ix_0^i)\\&amp;=\boldsymbol w^{\mathrm T}\boldsymbol x_i+b\end{align}则有\begin{align}|\boldsymbol w^{\mathrm T}\boldsymbol x_i+b|=||\boldsymbol w||d\\d=\cfrac{|\boldsymbol w^{\mathrm T}\boldsymbol x_i+b|}{||\boldsymbol w||}\end{align} 最大化间隔假设使样本中离超平面最近的样本点与超平面保证一定距离（可以证明，这样的超平面只有一个），令最近的样本点的函数间隔为$y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)$=1，这些最近的样本点被称为支持向量。则支持向量到超平面的几何间隔为：\begin{align}\gamma=\cfrac{1}{||\boldsymbol w||}\end{align}所以为了最大化间隔，目标函数为：\begin{align}\max\limits_{\boldsymbol w,b}&amp;\quad\cfrac{2}{||\boldsymbol w||}\\s.t.&amp;\quad y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)\geq1,\quad i=1,2,\cdots,m.\nonumber\end{align}为了求解方便目标函数（转化为凸优化问题），可将上式改为：$$\begin{align}\min\limits_{\boldsymbol w,b}&amp;\quad\cfrac{1}{2}||\boldsymbol w||^2\\s.t.&amp;\quad y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)\geq1,\quad i=1,2,\cdots,m.\nonumber\end{align}$$ 优化求解（对偶问题）求解凸优化问题可以用常用的二次规划优化包求解，也可以更有效的用拉格朗日乘数法求其对偶问题。 这样做的优点在于：一是对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。则该式的拉格朗日函数为(注意这里把约束条件变成了$g_i(x)\leq0$的形式)：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha)=\cfrac{1}{2}||\boldsymbol w||^2+\sum_{i=1}^{m}{\alpha_i(1-y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b))}\end{align}根据拉格朗日对偶性，构造函数：\begin{align}\theta_P(\boldsymbol w,b)=\max\limits_\alpha L(\boldsymbol w,b,\boldsymbol\alpha)=\cfrac{1}{2}||\boldsymbol w||^2\end{align}则原问题为:\begin{align}\min\limits_{\boldsymbol w,b}\cfrac{1}{2}||\boldsymbol w||^2=\min\limits_{\boldsymbol w,b}\max\limits_\alpha L(\boldsymbol w,b,\boldsymbol\alpha)\end{align}所以交换$min$,$max$得到其对偶问题为:\begin{align}\max\limits_\alpha \min\limits_{\boldsymbol w,b}L(\boldsymbol w,b,\boldsymbol\alpha)\end{align}对$L(\boldsymbol w,b,\boldsymbol\alpha)$求偏导并令其值为零。对$\boldsymbol w$，$b$有：\begin{align}&amp;\boldsymbol w+\sum_{i=1}^{m}{\alpha_i(0-\boldsymbol x_iy_i)}=0\\&amp;0+\sum_{i=1}^{m}{\alpha_i(0-y_i)}=0\end{align}化简得到：\begin{align}&amp;\boldsymbol w=\sum_{i=1}^{m}{\alpha_i\boldsymbol x_iy_i}\\&amp;0=\sum_{i=1}^{m}{\alpha_iy_i}\end{align}利用上述两式可以得到：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha)=&amp;\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\\&amp;\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_i\left(\left(\sum_{i=1}^m\alpha_jy_j\boldsymbol x_j\right)\cdot\boldsymbol x_i+b\right)\\&amp;=-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\end{align}因为在$\boldsymbol w$,$b$倒数为零时，$L(\boldsymbol w,b,\boldsymbol\alpha)$取得极小值，所以在约束条件$\sum_{i=1}^{m}{\alpha_iy_i}=0$下:\begin{align}\min\limits_{\boldsymbol w,b}L(\boldsymbol w,b,\boldsymbol\alpha)=-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\end{align}此时对偶问题可以写为:\begin{align}&amp;\max\limits_{\alpha}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.\quad &amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,\cdots,m\end{array}\end{align}也可以写成:\begin{align}&amp;\min\limits_{\alpha}\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}-\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.\quad &amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,\cdots,m\end{array}\end{align}关于这个式子的求解在下一篇(大概)，SMO算法中写上。下面看看为什么在边界的点叫做支持向量：KKT条件成立的式子有:\begin{align}\cfrac{\partial L(\boldsymbol w^{*},\alpha^{*},\beta^{*})}{\boldsymbol w}=0\\end{align}所以:\begin{align}\boldsymbol w^{*}=\sum_{i=1}^{m}{\alpha_i^{*}\boldsymbol x_iy_i}\end{align}则超平面可以写成:\begin{align}(\boldsymbol w^{*})^{\mathrm T}\boldsymbol x+b^{*}=\left(\sum_{i=1}^{m}{\alpha_i^{*}\boldsymbol x_iy_i}\right)^{\mathrm T}\boldsymbol x+b^{*}=\sum_{i=1}^m{\alpha_i^{*}y_i\boldsymbol x_i^{\mathrm T}\boldsymbol x}+b^{*}\end{align}从分离超平面的公式可以看到在计算时，只需要计算训练数据中对应于$\alpha_i^{*}\neq0$的点，而这些点满足$y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)-1=0$,即只有支持向量对SVM分离超平面的计算有用。SVM对于线性可分的数据是完美的，对于线性不可分问题(噪声，特异点)我们有Soft SVM（下节）。关于拉格朗日对偶一般优化问题可以写成以下形式:\begin{align}&amp;\min\limits_{x}f(x)\\&amp;s.t.\begin{cases}g_i(x)\leq0\\h_i(x)=0\end{cases}\end{align}根据拉格朗日方法，对应的拉格朗日函数则为：\begin{align}L(x,\alpha,\beta)=f(x)+\sum_i{\alpha_i}g_i(x)+\sum_i{\beta_ih_i(x)}\end{align}其中$\alpha$,$\beta$都是拉格朗日乘数，且要求$\alpha_i\geq0$，$\beta$的值任意。构造函数:\begin{align}\theta_P(x)=\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\end{align}由于$h_i(x)$为零，所以最后一项为零,又$g_i(x)\leq0$且$\alpha_i\geq0$,所以$L(x,\alpha,\beta)$最大值时,$\alpha=0$，此时$L(x,\alpha,\beta)=f(x)$,即$\theta_P(x)$等价于满足约束条件情况的的$f(x)$,$\theta_P(x)$可以表达为下式：\begin{array}\theta_P(x)=\begin{cases}f(x)\quad&amp;\text {$g_i(x)\leq0$&amp;$h_i(x)=0$}\\\+\infty\quad &amp;\text{$g_i(x)&gt;0$&amp;$h_i(x)\neq0$}\end{cases}\end{array}可以看到$\theta_P(x)$对原约束条件进行了吸收，使原来的约束优化问题变成了无约束优化问题，所以最初的优化问题可以写为:\begin{align}\min\limits_x\theta_P(x)=\min\limits_x\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\end{align}P代表primal，上式中的问题被称为原问题,和最开始的约束优化问题等价,将$min$和$max$交换顺序就变成了原问题的对偶问题,D代表dual：\begin{align}\max\limits_{\alpha,\beta:\alpha_i\geq0}\theta_D(\alpha,\beta)=\max\limits_{\alpha,\beta:\alpha_i\geq0}\min\limits_xL(x,\alpha,\beta)\end{align}注意这里自变量变成了$\alpha,\beta$。记$p^{*}$为原问题的最优解，对应最优解的最优变量取值为$x^{*}$。$d^{*}$为对偶问题的最优解，对应最优解的最优变量为$\alpha^{*}$，$\beta^{*}$。则有$d^{*}\leq p^{*}$。下面证明为什么有$d^{*}\leq p^{*}$,对于任意$\alpha$,$\beta$,$x$有\begin{align}\theta_D(\alpha,\beta)&amp;=\min\limits_xL(x,\alpha,\beta) \\&amp;\leq L(x,\alpha,\beta)\\&amp;\leq\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\\&amp;=\theta_P(\alpha,\beta)\\\end{align}又原问题和对偶问题都有最优解，所以：\begin{align}d^{*}&amp;=\max\limits_{\alpha,\beta:\alpha_i\geq0}\min\limits_xL(x,\alpha,\beta)\\&amp;=\max\limits_{\alpha,\beta:\alpha_i\geq0}\theta_D(\alpha,\beta)\leq\min\limits_x\theta_P(x)\\&amp;=\min\limits_x\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\\&amp;=p^{*}\end{align}Slater条件:指严格满足不等式约束条件,即$g(x){&lt;}0,(i=1,…,m)$,对于SVM问题，亦即要求“数据是可分的”,当数据不可分时,强对偶（Strong Duality）不成立。Strong Duality:原问题是Convex的，并且满足Slater条件，此时$\min\max L≥\max\min L$的等号成立。即$p^{*}=d^{*}$,这里需要注意一下，QP是凸优化问题的一种特殊情况。当“Strong Duality”关系成立时，一定存在$x^{*},\alpha^{*},\beta^{*}$,使得$x^{*}$是原问题的解，$\alpha^{*},\beta^{*}$是对偶问题的解。记primal, dual分别为原问题和对偶问题,那么有$p^{*}=d^{*}=L(x^{*}$,$\alpha^{*}\beta^{*}$成立，并且$x^{*},\alpha^{*},\beta^{*}$满足KKT条件(充分必要条件):\begin{align}\begin{array}\\\{\alpha_i}^{*}\geq0 \quad &amp;i=1,…,m\\\g_i(x^{*})\leq0 \quad &amp;i=1,…,m\\\{\alpha_i}^{*}g_i(x^{*})=0\quad &amp;i=1,…,m\\ \cfrac{\partial L(x^{*},\alpha^{*},\beta^{*})}{x_i}=0\quad &amp;i=1,…,m&amp;\\\\cfrac{\partial L(x^{*},\alpha^{*},\beta^{*})}{\beta_i}=h_i(x)=0\quad &amp;i=1,…,m&amp;\end{array}\end{align}其中式$\alpha_i^{*}g_i(x^{*})=0$被称为对偶互补条件,由此式可以知道，若$\alpha_i^{*}&gt;0$,则有$g_i(x^{*})=0$。对于SVM来说,这意味着只有离超平面最近的边界点（$g_i(x)=0$）其系数 $\alpha_i&gt;0$,对于其他的点都有$\alpha_i=0$。 参考文献https://www.cnblogs.com/dreamvibe/p/4349886.htmlhttps://blog.csdn.net/diligent_321/article/details/53396682]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之LR(逻辑回归)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteLR%2F</url>
    <content type="text"><![CDATA[逻辑回归是一个回归函数，但是应用在分类问题上，可以对事件发生的概率进行预测。Todo Lost： 多元线性回归参数估计推导 为什么LR选择Sigmoid函数？ 梯度下降法的向量化？ 线性回归线性回归公式：$$f(x_i)=\omega x_i+b$$目的是使$f(x_i)$尽可能的靠近$y_i$(实际值). 求解$\omega$和$b$一般采均方误差最小化，这种方法又被称为“最小二乘法”，即找到一条直线使所有点到直线的欧氏距离最小，即最小化Cost Function: 最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form，而非线性最小二乘没有closed-form，通常用迭代法求解。 $$\begin{array}\left(\omega^*,b^*)&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(f(x_i)-u_i)^2}\\&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(y_i-\omega x_i-b)^2}\end{array}$$ 求解$\omega$和$b$，将上式分别对$\omega$和$b$求导有(即求偏导)： $$\omega =\cfrac{\sum\limits_{i=1}^m{y_i(x_i-\bar x)}}{\sum\limits_{i=1}^m{x_i^2-\cfrac1m\bigg(\sum\limits_{i=1}^m{x_i}\bigg)^2}}$$$$b=\cfrac1m\sum_{i=1}^m(y_i-\omega x_i)$$ 上述是一元线性回归的情况.对于多元线性回归则有： $$f(\boldsymbol x_i)=\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b$$或者写作$$ h_\theta(\boldsymbol x_i)=f(\theta^{\mathrm T}\boldsymbol x_i)=\cfrac1{1+\mathrm e^{-\theta^{\mathrm T}\boldsymbol x_i}}$$使$f(\boldsymbol x_i)$尽可能的靠近$y_i$(实际值） 多元线性回归参数待补完 Logistic Regression（对数几率回归，逻辑斯蒂回归）LR概念LR的思想是将线性回归输出的连续值$\left(-\infty,+\infty \right)$映射到（0,1）的一个概率值，从而能够根据阈值来进行分类。Sigmoid函数就正好满足需求，他的定义如下： Sigmoid函数是一个任意阶可导凸函数。$$y=\cfrac{1}{1+\mathrm{e}^{-z}}$$为什么选择Sigmoid函数？？？？？？ 则把线性回归的输出带入Sigmoid函数则有：$$\hat y=\cfrac{1}{1+\mathrm{e}^{-(\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b)}}$$ LR参数估计两种求解LR参数的方法，极大似然估计和梯度下降法。首先是LR的Cost function,如果使用和线性回归一样的损失函数会得到一个非凸函数，不利于后续的求解（比如用梯度下降法得到的就不一定是最优解）。LR的损失函数定义是($\hat y$为预测值，$y$为实际值)：$$\begin{array}\\J(\boldsymbol \omega,b)&amp;=&amp;\cfrac1m\sum_{i=1}^{m}L\left({\hat y_i}-y_i\right)\\&amp;=&amp;-\cfrac{1}{m}\sum_{i+1}^{m}[y_i\log(\hat y_i)+(1-y_i)log(1-\log(\hat y_i))]\end{array}$$ 关于LR的Cost Function我的理解:首先是合理性上，从公式看当$y^{(i)}=1$时，则损失函数可以简化为$J(\omega,b)=-\log({\hat y^{(i)}})$，则当$\hat y^{(i)}\to0$时，$J\to\infty$,即Cost趋近于无穷。当$\hat y^{(i)}\to1$时，$J\to 0$,即Cost趋近于0.$y^{(i)}=0$时，同理。所以这个Cost Function从概念上是符合我们的要求的。推导：将$y$视作是样本为正例的概率，则$1-y$是反例的概率,即：$$\begin{array}\\P(y=1|\boldsymbol x)=\hat y \\P(y=0|\boldsymbol x)=1-\hat y\end{array}$$结合上述二式则有：$$P(y|\boldsymbol x;\boldsymbol\omega,b)=\hat y^y(1-\hat y)^{(1-y)}$$有了条件概率公式我们则可以用极大似然估计求解参数，取$P(y|\boldsymbol x;\boldsymbol\omega,b)$在整个训练集上的似然函数有：$$L(\boldsymbol \omega,b)=\prod_{i=1}^{m}{\left[(\hat y_i)^{y_i}(1-\hat y_i)^{(1-y_i)}\right]}$$对似然函数取对数有：$$l(\boldsymbol\omega,b )=\mathrm{log}L(\boldsymbol \omega,b)=\sum_{i=1}^{m}{[y_i\mathrm{log}(\hat y _i)+(1-y_i)\mathrm{log}(1-\hat y_i)]}$$极大似然估计就是要求得使$l(\boldsymbol \omega,b)$最大时的$\boldsymbol\omega$和$b$的值，这里可以用极大似然估计本身的解法或者梯度上升法解决。而cost function值越小越好，所以Andrew Ng的课件里在$l(\boldsymbol \omega,b)$前乘了个系数$-\cfrac1m$当做最终的cost fucntion：$$J(\boldsymbol \omega,b)=-\cfrac1ml(\boldsymbol\omega,b )$$这样一来求使cost fucntion最小的$（\boldsymbol\omega,b）$就等于求解在给定样本情况下极大似然估计。 梯度下降法最最开始的时候没搞清楚梯度下降法和最小二乘法的区别，梯度下降法是求解非线性最小二乘法（没有闭式解）的方法之一。 为方便公式书写令$\boldsymbol\theta=(\boldsymbol\omega,b)$,则对$\boldsymbol\theta$的第$j$个属性按照梯度下降法更新则有： 这部分公式中的有的粗体用错了，之后有时间修正!!!!$$\theta_j=\theta_j-\alpha\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}$$具体梯度下降法过程推导如下：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac{\partial }{\theta_j}\cfrac1m\sum_{i=1}^m{[y_i\mathrm log(h_{\theta}(\boldsymbol x_i))+(1-y_i)\mathrm log(1-h_\theta(\boldsymbol x_i))]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}h_\theta(\boldsymbol x_i)-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}{h_\theta(\boldsymbol x_i)}\right]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)}\end{array}$$而$$\begin{array}\\\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)&amp;=&amp;\cfrac{\partial}{\partial\theta_j}\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\\&amp;=&amp;\cfrac{\mathrm e^{-\theta^\mathrm T\boldsymbol x_i}}{(1+\mathrm e^{-\theta^\mathrm T\boldsymbol x_i})^2}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}-1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\end{array}$$其中$x_{i,j}$表示第i个样本中的第j个属性,且$h_\theta(\boldsymbol x_i)=g(\theta^{\mathrm T}\boldsymbol x_i)$所以：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]}g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i(1-g(\theta^{\mathrm T}\boldsymbol x_i))-(1-y_i)g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i-g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp; \cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}\end{array}$$所以$\theta_j$的更新公式可以写作：$$\theta_j=\theta_j-\alpha\cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}$$ 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>LR</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之SVM2(支持向量机)]]></title>
    <url>%2F2018%2F04%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8BSVM2(%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)%2F</url>
    <content type="text"><![CDATA[这一节写软间隔最大化支持向量机和核技巧。 Soft SVMSVM有时候会遇到数据线性不可分的问题,线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件,为了解决这个问题，我们使用Soft SVM,即在原来的超平面方程中加入松弛变量$\xi\geq0$允许某些样本可以在原约束条件下犯错,使函数间隔加上这个变量后大于等于1,即约束条件变为:\begin{align}y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b)\geq1-\xi_i\\\xi\geq0,i=1,…,m\end{align}样本中每一个变量对应一个松弛变量，此时目标函数变为(对于每一个松弛变量$\xi_i$,支付一个代价$\xi_i$):\begin{align}L(\boldsymbol w,\xi)=\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\end{align}这里的$C$是惩罚参数,$C$越大对于误分类的惩罚越大。最小化上式则表示在最大化间隔同时使误分类的样本数目尽可能的少。 Soft SVM的原始问题可以写作:\begin{align}&amp;\min\limits_{\boldsymbol w,b,\boldsymbol\xi_i}\quad\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\\&amp;\begin{array}\\s.t.\quad &amp;y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b)\geq1-\xi_i\\&amp;\xi\geq0,i=1,…,m\end{array}\end{align}上式依旧是一个凸二次规划问题，按照拉格朗日乘数法，得到拉格朗日函数为：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha,\boldsymbol\xi,\boldsymbol\mu)=&amp;\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\\&amp;+\sum_{i=1}^m{\alpha_i(1-\xi_i-y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b))}-\sum_{i=1}^m\mu_i\xi_i\end{align}$\alpha_i$,$\mu_i$均为拉格朗日系数,和上节一样可以得到对偶问题为:\begin{align}\max\limits_{\alpha_i,\mu_i}\min\limits_{\boldsymbol w,b,\boldsymbol\xi}L(\boldsymbol w,b,\boldsymbol\alpha,\boldsymbol\xi,\boldsymbol\mu)\end{align} 求拉格朗日函数的偏导有:\begin{align}\boldsymbol w&amp;=\sum_{i=1}^m\alpha_iy_i\boldsymbol x_i\\0&amp;=\sum_{i=1}^m\alpha_iy_i\\C&amp;=\alpha_i+\mu_i\end{align}把上述式子带入对偶问题的函数化简:\begin{align}&amp;\max\limits_{\alpha_i}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.&amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,…,m\\&amp;0\leq\alpha_i\leq C\\\end{array}\end{align}同样类似于硬间隔SVM，软间隔支持向量机也满足KKT条件,则从上式可以得到, 若$\alpha_i=0$,则这些样本对模型不会产生影响。 若$0&lt;\alpha_i&lt; C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i&gt;0$,从而有$\xi_i=0$即这些点位于最大间隔边界上,就是我们的支持向量。 若$\alpha_i=C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i=0$,此时若$0&lt;\xi_i&lt;1$,则分类正确，样本点在间隔边界和分离超平面之间。若$\xi_i=1$，则分类正确，样本点在分离超平面上。 若$\alpha_i=C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i=0$,此时若$\xi_i&gt;1$,则分类错误，样本点位于分类超平面另一边。所以最大间隔边界上的样本点可能是支持向量，也有可能是离群点。软间隔支持向量机的最终模型仍然仅与支持向量有关，保持了稀疏性。 损失函数常见的代替损失函数 hinge损失(hinge loss):$l_{hinge}(z)=max(0,1-z)$ 指数损失(exponential loss):$l_{exp}(z)$=exp(-z) 对率损失(logistic loss):$l_{log}=log(1+exp(-z))$Liner SVM用hinge损失。 核函数有时候数据在原始样本空间线性不可分,但可以将样本从原始空间映射到一个更高维的特征空间(希尔伯特空间),使样本在这个空间内线性可分。而且如果原始空间是有限维，即属性有限，那么一定存在一个高维特征空间使样本线性可分。把原始空降映射到高维空间叫做kernel trick核技巧，同时不会增加计算量。 我们假设$\phi(\boldsymbol x)$为讲原始空间的样本$\boldsymbol x$映射后的特征向量。 与之前推导一样我们可以得到对偶问题为:\begin{align}&amp;\max\limits_{\alpha_i}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.&amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,…,m\\\end{array}\end{align}在高维空间计算$\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)$通常比较困难,设计这样一个函数:\begin{align}\kappa(\boldsymbol x_i,\boldsymbol x_j)=\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)\end{align}即$\boldsymbol x_i$与$\boldsymbol x_j$在高维空间的内积等于他们在原始样本空间通过函数$\kappa(\boldsymbol x_i,\boldsymbol x_j)$计算的结果，这个技巧叫核技巧，这个函数就是核函数。 核技巧的思想是不显示的表示映射函数,而是直接计算核函数。这样最终求解超平面方程为:\begin{align}f(\boldsymbol x)=\sum\limits_{i=1}^m{\alpha_iy_i\kappa(\boldsymbol x_,\boldsymbol x_i)}+b\end{align}如何找到核函数？定理:只要一个堆成函数所对应的和核矩阵半正定，他就能作为核函数使用。 常用核函数 线性核 $\kappa(\boldsymbol x_i,\boldsymbol x_j)=\boldsymbol x_i^{\mathrm T}\boldsymbol x_j$ 多项式核 $\kappa(\boldsymbol x_i,\boldsymbol x_j)=(\boldsymbol x_i^{\mathrm T}\boldsymbol x_j)^d\quad d\geq1$为多项式次数 高斯核$\kappa(\boldsymbol x_i,\boldsymbol x_j)=exp\left(-\cfrac{||\boldsymbol x_i-\boldsymbol x_j||^2}{2\sigma^2}\right)\quad\sigma&gt;0$为高斯核带宽 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
        <tag>soft SVM</tag>
        <tag>核技巧</tag>
      </tags>
  </entry>
</search>
