<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[秋招练手笔记]]></title>
    <url>%2F2018%2F07%2F03%2F%E5%89%91%E6%8C%87offer%2F</url>
    <content type="text"><![CDATA[秋招练手的一些笔记。 NO.2 替换空格题目描述 请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 第一个版本 未实现在原字符串上的直接修改（会报数组越界）。新建了一个字符串s1 从后向前替换，避免多个空格时，后面的字符会移动多次的情况，每个字符只需移动一次 不考虑java里面的replace函数 123456789101112131415161718192021222324252627282930public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int count = 0; char[] s = str.toString().toCharArray(); for(int i=0;i&lt;s.length;i++) &#123; if(s[i]==' ')&#123; count++; &#125; &#125; char[] s1 = new char[2*count + str.length()]; int loc=2*count + str.length()-1; for(int i=s.length-1;i&gt;=0;i--) &#123; if(s[i]!=' ') &#123; s1[i+count*2]=s[i]; &#125; else &#123; count--; s1[i+2*count]='%'; s1[i+count*2+1]='2'; s1[i+count*2+2]='0'; &#125; &#125; return String.valueOf(s1); &#125;&#125; NO.3 从尾到头打印链表题目描述 输入一个链表，从尾到头打印链表每个节点的值。 后进先出，翻转，联想到堆栈 利用堆栈先进后出的特性，链表的头先进则最后出，即将链表翻转，实现从尾到头 还有递归解法 12345678910111213141516171819202122232425262728/*** public class ListNode &#123;* int val;* ListNode next = null;** ListNode(int val) &#123;* this.val = val;* &#125;* &#125;**/import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack=new Stack&lt;Integer&gt;(); while(listNode!=null)&#123; stack.push(listNode.val); listNode=listNode.next; &#125; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); while(!stack.isEmpty()) &#123; list.add(stack.pop()); &#125; return list; &#125;&#125; NO4. 重建二叉树题目描述 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8} 和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 递归实现，通过前序遍历确定根节点，中序遍历确定左右子树 见剑指offer p63 1234567891011121314151617181920212223242526/** * Definition for binary tree * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */import java.util.*;public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; if(pre.length == 0 || in.length == 0)&#123; return null; &#125; TreeNode tn = new TreeNode(pre[0]); for (int i = 0;i &lt; in.length;i++)&#123; if(pre[0] == in[i])&#123; tn.left = reConstructBinaryTree(Arrays.copyOfRange(pre,1,i+1),Arrays.copyOfRange(in,0,i)); tn.right = reConstructBinaryTree(Arrays.copyOfRange(pre,i+1,pre.length),Arrays.copyOfRange(in,i+1,in.length)); &#125; &#125; return tn; &#125;&#125; NO5. 用两个栈实现队列题目描述 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 用stack1当做队列的入，stack当做队列的出 每次stack1不为空时，将其所有元素出栈并压入stack2，这时stack2出栈即为先进先出 每次stack2完成出栈操作后如果栈不为空则需要将所有数重新压进空的stack1，等待新的数据压进stack1，保证stack2的剩余数据比新数据早压入stack1 123456789101112131415161718192021import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; while(!stack1.empty())&#123; stack2.push(stack1.pop()); &#125; int i=stack2.pop(); while(!stack2.empty())&#123; stack1.push(stack2.pop()); &#125; return i; &#125;&#125; NO6. 旋转数组的最小数字题目描述 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 解法一 最普通的方式直接遍历数组找到最小值，没有利用旋转和排序数组的特性。复杂度$O(n)$12345678910111213141516171819import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; if(array.length==0) return 0; else &#123; int min=array[0]; for(int i=0;i&lt;array.length-1;i++) &#123; if(min&gt;array[i+1]) &#123; min=array[i]; &#125; &#125; return min; &#125; &#125;&#125; 解法二 稍微优化一下,利用旋转特性,旋转分界出前一个元素大于后一个元素，如果没有旋转则输出第一个值，复杂度还是$O(n)$ 12345678910111213141516import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; if(array.length==0) return 0; else &#123; for(int i=0;i&lt;array.length-1;i++) &#123; if(array[i]&gt;array[i+1]) return array[i+1]; &#125; &#125; return array[0]; &#125;&#125; 解法三 二分法查找待补完 NO7. 斐波那契数列题目描述 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。 解法一 菲波那切数列第$N$项的值等于第$N-1$和$N-2$项的和，可以用递归解决 递归存在严重的效率问题，在递归树中有很多节点重复，时间复杂度以$n$的指数倍增加。 123456789 public class Solution &#123; public int Fibonacci(int n) &#123; if(n&lt;=0) return 0; if(n&lt;=1) return 1; return Fibonacci(n-1)+Fibonacci(n-2); &#125;&#125; 解法二 循环解决问题，从前往后，注意特殊项1,212345678910111213141516171819202122public class Solution &#123; public int Fibonacci(int n) &#123; int fibN_0 = 1; int fibN_1 = 1; int fibN_N=0; if(n==0) return 0; else if (n==1||n==2) return 1; else&#123; for(int i=3;i&lt;=n;i++) &#123; fibN_N=fibN_1 + fibN_0; fibN_0=fibN_1; fibN_1=fibN_N; &#125; return fibN_N; &#125; &#125;&#125; NO8. 跳台阶1题目描述 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 $1$阶或者$2$阶，那么假定第一次跳的是一阶，那么剩下的是$n-1$个台阶，跳法是$f(n-1)$;假定第一次跳的是$2$阶，那么剩下的是$n-2$个台阶，跳法是$f(n-2)$,由假设可以得出总跳法为: $f(n)=f(n-1)+f(n-2)$.然后通过实际的情况可以得出：只有一阶的时候 $f(1) = 1$ ,只有两阶的时候可以有 $f(2) = 2$.可以发现最终得出的是一个斐波那契数列 注意这个数列前两项与$NO.8$的有区别。 NO9. 跳台阶2题目描述 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 由NO8的分析，$f(n)=f(n-1)+f(n-2)+···+f(0)$,$f(n-1)=f(n-2)+f(n-3)+···+f(0)$，则结合两式由$f(n)=2*f(n-1)$,则可以递归解决，也可以知道这是一个等比数列$f(n)=2^{n-1}$123456789public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==1) return 1; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125; 12345public class Solution &#123; public int JumpFloorII(int target) &#123; return (int)Math.pow(2,target-1); &#125;&#125; NO10. 矩形覆盖题目描述 我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 归纳总结发现还是一个斐波那契额数列，跟之前解法一样，不付代码 NO11. 数值的整数次方题目描述 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表。 解法一 用逻辑左移避免负数最高位在算数左移时补一造成的死循环。12345678910111213public class Solution &#123; public int NumberOf1(int n) &#123; int count=0; while(n!=0) &#123; if((n&amp;1)!=0)&#123; count++; &#125; n=n&gt;&gt;&gt;1; &#125; return count; &#125;&#125; 解法二 用1不停左移检测每一位 注意$flag！=0$这个判断会在越界时为0，也意味着每次都得循环32或者更多次(取决于系统) 注意 if((n&amp;flag)!=0) 这个判断条件不能写成if((n&amp;flag)==0)，因为按位与的时候当前位为1或0，但结果不一定。 1234567891011121314public class Solution &#123; public int NumberOf1(int n) &#123; int count=0; int flag=1; while(flag!=0) &#123; if((n&amp;flag)!=0)&#123; count++; &#125; flag=flag&lt;&lt;1; &#125; return count; &#125;&#125; NO11. 解法三 剑指offer P102 解法四 非常巧妙的一种做法将数字转成字符串，再转成字符数组，统计数组中1的个数。 123456789101112public class Solution &#123; public int NumberOf1(int n) &#123; int t=0; char[]ch=Integer.toBinaryString(n).toCharArray(); for(int i=0;i&lt;ch.length;i++)&#123; if(ch[i]=='1')&#123; t++; &#125; &#125; return t; &#125;&#125; 数值的整数次方题目描述 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 解法一 注意特殊情况 其实还缺一个特例的判断，即base为0的时候，这时double不能用if(base==0)来判断 1234567891011121314public class Solution &#123; public double Power(double base, int exponent) &#123; double result=1; if(exponent==0) return 1; for(int i=0;i&lt;Math.abs(exponent);i++)&#123; result*=base; &#125; if(exponent&lt;0)&#123; result=1/result; &#125; return result; &#125;&#125; 解法二 LEETCODEno.1 TWO SUM给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1] 解法一 暴力搜索，复杂度$O(n^2)$,第一时间反应的解法。 123456789101112131415class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; int[] result=new int[2]; for(int i=0;i&lt;nums.length-1;i++) for(int j=i+1;j&lt;nums.length;j++) &#123; if(nums[i]+nums[j]==target)&#123; result[0]=i; result[1]=j; return result; &#125; &#125; return result; &#125;&#125; 解法二 利用hashmap查询速度时间复杂度一般为$O(1)$的特点，减少时间复杂度。 hashmap由于key-value对的特性，所以查找速度快。 map.put(nums[i], i),传进key和value参数 Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;()新建一个HashMap map.containsKey(complement)找到对应的数组索引 map.get(complement)取对应的数组索引 throw new IllegalArgumentException(&quot;No two sum solution&quot;)没有返回值时抛出异常 12345678910111213public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; map.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement) &amp;&amp; map.get(complement) != i) &#123; return new int[] &#123; i, map.get(complement) &#125;; &#125; &#125; throw new IllegalArgumentException(&quot;No two sum solution&quot;);&#125;]]></content>
      <categories>
        <category>笔试</category>
      </categories>
      <tags>
        <tag>秋招</tag>
        <tag>笔试</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔试报告_蚂蚁金服]]></title>
    <url>%2F2018%2F06%2F19%2F%E7%AC%94%E8%AF%95%E6%8A%A5%E5%91%8A_%E6%99%8F%E5%AE%87%2F</url>
    <content type="text"><![CDATA[拿到数据以发现特征只有一列,即资金流的流入流出情况,第一反应是惯性思维用熟悉的LSTM能不能解决问题,但也明白无论怎么从原有数据构造新特征在LSTM上的表现都应该不是很好，花了几个小时处理数据写完模型发现无论如何调整参数构造特征，模型都无法收敛而且过拟合严重,时间有限也无法严格验证LSTM是否有严格的可行性且对该数据业务不了解，所以放弃原有方案采用之前有了解过的符合该数据的ARIMA模型. 模型及方案选择本次建模根据数据特征（时间序列数据，尝试LSTM和ARIMA模型，选择了时间序列模型ARIMA来预测渠道7月24日至7月28日每天从当日0点到24点的流入流出轧差值（总流入-总流出）。 选择ARIMA方案的理由如下： - 数据经检验为平稳模型，符合使用ARIMA模型的要求。 - 有很多成功解决此类型问题的先例证明此模型的可靠性和有效性。 - 特征过少不宜使用传统机器学习模型。 - 模型较为简单易懂，在有限的笔试时间里我可以迅速上手。 由于24日-28日的数据部分已经给出，即24日-28日每天10am前的数据已知。所以预测方案有两个： 1. 直接舍弃24日至28日的数据，以天为单位统计每日流入流出轧差值，并以此建模，直接预测24日-28日每日流入流出轧差值。 2. 不舍弃24日至28日的数据,以小时为单位统计计算每小时的流入流出，以此建模，预测24日-28日每日10am后的小时流入流出情况，并和当日已知流入流出情况汇总，得到最终的每日流出轧差值的预测值。 从业务方面考虑，初步认为流入流出轧差值以天为细粒度较为合理，理由如下： 1. 小时细粒度具有极大的随机性，突发金额流入流出的情况会影响模型的准确性。 2.一般业务情境中，资金流入流出以天为细粒度具有规律性（数据中也可以大致看出），突发情况较少。 3.月细粒度会导致数据过少，建模困难。 预测结果及分析由于初次上手ARIMA模型，主要时间花在参数了解和写模型上，参数未能精细调整，可能会影响部分结果，我会在后面写出可能的改进方案。 数据处理由于不了解业务，无法对数据中的异常值进行判断，对于5个缺失值选择用每五分钟的流入流出均值代替，有3340个时间点同时有$RE$和$SE$两种amount，业务情况不清楚所以暂不处理。对$RE$的值标记为负。之后按天聚合得到的数据如图（去除24-29日的数据）： 上图基本显示出了一定的平稳性，在一个数值左右波动（如果想更明确，可以计算一维差分来用），通过Dickey-Fuller Test来检验其平稳性,结果如下： Results of Dickey-Fuller Test: Test Statistic -4.917220 p-value 0.000032 #Lags Used 1.000000 Number of Observations Used 202.000000 Critical value (1%) -3.463144 Critical value (5%) -2.875957 Critical value (10%) -2.574455 dtype: float64 Test Statistic小于置信度为1%时的值，且p值远小于0.05，说明序列平稳。 接下来通过$ACF$,$PACF$进行$ARIMA（p，d，q)$的$p$，$q$参数估计： 参数$d=0$为差分的阶数，从$ACF$,$PACF$图中得到$p=9$,$q=3$. 带入$ARIMA$模型后得到,24日前的历史模拟情况对比： 预测24日-28日的情况如图： 24日-28日对应的资金流入流出轧差值的预测值为： 07.24 248680.669874 07.25 223628.952776 07.26 212744.495103 07.27 201486.709855 07.28 190364.054472 结果说明及可能的改进现学了$ARIMA$中间肯定存在理解失误或者细节调整有问题的地方。从历史拟合数据看，均方根误差较大存在一定的误差，但数据大体趋势正确。 可能的改进方案如下： 1.依然沿用ARIMA这个解决方案，但调整细节及相关参数（误差白噪声检查，BIC确定p，q值等）。 2.收集更多维度的特征（如交易笔数，交易渠道等），换用LSTM，RNN等深度学习或者机器学习的时序模型 3.花更多的时间去寻求业务上的规律 4.寻求其他的计量经济学模型并进行模型融合以减少误差 参考资料 https://www.cnblogs.com/bradleon/p/6827109.html https://blog.csdn.net/gdyflxw/article/details/55509656]]></content>
      <categories>
        <category>笔试</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>ARIMA</tag>
        <tag>时间序列分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成方法与判定方法]]></title>
    <url>%2F2018%2F04%2F25%2FGenApp-and-DisApp%2F</url>
    <content type="text"><![CDATA[监督学习方法分生成方法与判定方法，这篇文章是关于二者理解的笔记。 李航老师的《《统计学习方法》P18:生成方法由数据学习联合概率分布$P(X,Y)$,然后再求条件概率分布作为预测模型。典型的生成模型有：朴素贝叶斯，隐马尔科夫模型。判别方法有数据直接学习决策函数$f(X)$或者条件概率分布$P(XY)$作为预测模型。典型的判定模型有：KNN，决策树，LR，SVM等。 生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。 判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 生成方法对于历史数据有多少个类别就要生成多少个模型，然后用新数据特征去匹配这些模型，取最符合的。 判别方法根据历史数据直接生成一个判别式，新数据进来直接就可以判断他属于哪一个类别。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之LR(逻辑回归)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteLR%2F</url>
    <content type="text"><![CDATA[逻辑回归是一个回归函数，但是应用在分类问题上，可以对事件发生的概率进行预测。Todo Lost： 多元线性回归参数估计推导 为什么LR选择Sigmoid函数？ 梯度下降法的向量化？ 线性回归线性回归公式：$$f(x_i)=\omega x_i+b$$目的是使$f(x_i)$尽可能的靠近$y_i$(实际值). 求解$\omega$和$b$一般采均方误差最小化，这种方法又被称为“最小二乘法”，即找到一条直线使所有点到直线的欧氏距离最小，即最小化Cost Function: 最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form，而非线性最小二乘没有closed-form，通常用迭代法求解。 $$\begin{array}\left(\omega^*,b^*)&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(f(x_i)-u_i)^2}\\&amp;=&amp; \mathop{\arg\min}\limits_{(\omega,b)}\sum_{i=1}^m{(y_i-\omega x_i-b)^2}\end{array}$$ 求解$\omega$和$b$，将上式分别对$\omega$和$b$求导有(即求偏导)： $$\omega =\cfrac{\sum\limits_{i=1}^m{y_i(x_i-\bar x)}}{\sum\limits_{i=1}^m{x_i^2-\cfrac1m\bigg(\sum\limits_{i=1}^m{x_i}\bigg)^2}}$$$$b=\cfrac1m\sum_{i=1}^m(y_i-\omega x_i)$$ 上述是一元线性回归的情况.对于多元线性回归则有： $$f(\boldsymbol x_i)=\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b$$或者写作$$ h_\theta(\boldsymbol x_i)=f(\theta^{\mathrm T}\boldsymbol x_i)=\cfrac1{1+\mathrm e^{-\theta^{\mathrm T}\boldsymbol x_i}}$$使$f(\boldsymbol x_i)$尽可能的靠近$y_i$(实际值） 多元线性回归参数待补完 Logistic Regression（对数几率回归，逻辑斯蒂回归）LR概念LR的思想是将线性回归输出的连续值$\left(-\infty,+\infty \right)$映射到（0,1）的一个概率值，从而能够根据阈值来进行分类。Sigmoid函数就正好满足需求，他的定义如下： Sigmoid函数是一个任意阶可导凸函数。$$y=\cfrac{1}{1+\mathrm{e}^{-z}}$$为什么选择Sigmoid函数？？？？？？ 则把线性回归的输出带入Sigmoid函数则有：$$\hat y=\cfrac{1}{1+\mathrm{e}^{-(\boldsymbol \omega^ \mathrm{T}\boldsymbol x_i+b)}}$$ LR参数估计两种求解LR参数的方法，极大似然估计和梯度下降法。首先是LR的Cost function,如果使用和线性回归一样的损失函数会得到一个非凸函数，不利于后续的求解（比如用梯度下降法得到的就不一定是最优解）。LR的损失函数定义是($\hat y$为预测值，$y$为实际值)：$$\begin{array}\\J(\boldsymbol \omega,b)&amp;=&amp;\cfrac1m\sum_{i=1}^{m}L\left({\hat y_i}-y_i\right)\\&amp;=&amp;-\cfrac{1}{m}\sum_{i+1}^{m}[y_i\log(\hat y_i)+(1-y_i)log(1-\log(\hat y_i))]\end{array}$$ 关于LR的Cost Function我的理解:首先是合理性上，从公式看当$y^{(i)}=1$时，则损失函数可以简化为$J(\omega,b)=-\log({\hat y^{(i)}})$，则当$\hat y^{(i)}\to0$时，$J\to\infty$,即Cost趋近于无穷。当$\hat y^{(i)}\to1$时，$J\to 0$,即Cost趋近于0.$y^{(i)}=0$时，同理。所以这个Cost Function从概念上是符合我们的要求的。推导：将$y$视作是样本为正例的概率，则$1-y$是反例的概率,即：$$\begin{array}\\P(y=1|\boldsymbol x)=\hat y \\P(y=0|\boldsymbol x)=1-\hat y\end{array}$$结合上述二式则有：$$P(y|\boldsymbol x;\boldsymbol\omega,b)=\hat y^y(1-\hat y)^{(1-y)}$$有了条件概率公式我们则可以用极大似然估计求解参数，取$P(y|\boldsymbol x;\boldsymbol\omega,b)$在整个训练集上的似然函数有：$$L(\boldsymbol \omega,b)=\prod_{i=1}^{m}{\left[(\hat y_i)^{y_i}(1-\hat y_i)^{(1-y_i)}\right]}$$对似然函数取对数有：$$l(\boldsymbol\omega,b )=\mathrm{log}L(\boldsymbol \omega,b)=\sum_{i=1}^{m}{[y_i\mathrm{log}(\hat y _i)+(1-y_i)\mathrm{log}(1-\hat y_i)]}$$极大似然估计就是要求得使$l(\boldsymbol \omega,b)$最大时的$\boldsymbol\omega$和$b$的值，这里可以用极大似然估计本身的解法或者梯度上升法解决。而cost function值越小越好，所以Andrew Ng的课件里在$l(\boldsymbol \omega,b)$前乘了个系数$-\cfrac1m$当做最终的cost fucntion：$$J(\boldsymbol \omega,b)=-\cfrac1ml(\boldsymbol\omega,b )$$这样一来求使cost fucntion最小的$（\boldsymbol\omega,b）$就等于求解在给定样本情况下极大似然估计。 梯度下降法最最开始的时候没搞清楚梯度下降法和最小二乘法的区别，梯度下降法是求解非线性最小二乘法（没有闭式解）的方法之一。 为方便公式书写令$\boldsymbol\theta=(\boldsymbol\omega,b)$,则对$\boldsymbol\theta$的第$j$个属性按照梯度下降法更新则有： 这部分公式中的有的粗体用错了，之后有时间修正!!!!$$\theta_j=\theta_j-\alpha\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}$$具体梯度下降法过程推导如下：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac{\partial }{\theta_j}\cfrac1m\sum_{i=1}^m{[y_i\mathrm log(h_{\theta}(\boldsymbol x_i))+(1-y_i)\mathrm log(1-h_\theta(\boldsymbol x_i))]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}h_\theta(\boldsymbol x_i)-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\cfrac{\partial}{\partial\theta_j}{h_\theta(\boldsymbol x_i)}\right]}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)}\end{array}$$而$$\begin{array}\\\cfrac{\partial}{\partial\theta_j}g(\theta^{\mathrm T}\boldsymbol x_i)&amp;=&amp;\cfrac{\partial}{\partial\theta_j}\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\\&amp;=&amp;\cfrac{\mathrm e^{-\theta^\mathrm T\boldsymbol x_i}}{(1+\mathrm e^{-\theta^\mathrm T\boldsymbol x_i})^2}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;\cfrac{1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}-1}{1+\mathrm{e}^{-\theta^{\mathrm T}\boldsymbol x_i}}\cfrac{\partial}{\partial\theta_j}\theta^{\mathrm T}\boldsymbol x_i\\&amp;=&amp;g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\end{array}$$其中$x_{i,j}$表示第i个样本中的第j个属性,且$h_\theta(\boldsymbol x_i)=g(\theta^{\mathrm T}\boldsymbol x_i)$所以：$$\begin{array}\\\cfrac{\partial J(\boldsymbol\theta)}{\partial\theta_j}&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i\cfrac{1}{h_\theta(\boldsymbol x_i)}-(1-y_i)\cfrac1{1-h_\theta(\boldsymbol x_i)}\right]}g(\theta^{\mathrm T}\boldsymbol x_i)(1-g(\theta^{\mathrm T}\boldsymbol x_i))x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i(1-g(\theta^{\mathrm T}\boldsymbol x_i))-(1-y_i)g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp;-\cfrac1m\sum_{i=1}^m{\left[y_i-g(\theta^{\mathrm T}\boldsymbol x_i)\right]}x_{i,j}\\&amp;=&amp; \cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}\end{array}$$所以$\theta_j$的更新公式可以写作：$$\theta_j=\theta_j-\alpha\cfrac1m\sum_{i=1}^m{\left[h_\theta(\boldsymbol x_i)-y_i\right]}x_{i,j}$$ 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>笔记</tag>
        <tag>LR</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之SVM(支持向量机)]]></title>
    <url>%2F2018%2F04%2F25%2FNoteSVM(1)%2F</url>
    <content type="text"><![CDATA[支持向量机是一种二分类模型，是特征空间上间隔最大化的线性分类器。这篇是关于SVM的笔记。 Todo list： 算法原理 算法推倒 算法优化 算法实现 一些疑问 基于训练集，SVM想找到一个最优划分超平面将不同的类别分开，学习的策略是使间隔最大化（之后说明），则该超平面可以写成： $$\boldsymbol w^\mathrm{T}\boldsymbol x+b=0$$ 其中$\boldsymbol w$为超平面法向量决定超平面方向，$b$为位移项决定超平面与原点的距离。令$f(\boldsymbol x)=\boldsymbol w^\mathrm{T}\boldsymbol x+b$则$f(\boldsymbol x)=0$时表示$\boldsymbol x$在超平面上。 样本数据线性可分的情况下，我们假设当$f(\boldsymbol x)&gt;0$时令其所对应的$y=+1$,当$f(\boldsymbol x)&lt;0$时令其所对应的$y=-1$ SVM为什么采用间隔最大化？当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。 几何间隔与函数间隔函数间隔的定义如下，对于某一样本点$\boldsymbol x_i$：\begin{align}\hat\gamma_i=yf(\boldsymbol x)=y(\boldsymbol w^{\mathrm{T}}\boldsymbol x+b)\end{align}函数间隔保证自身的非负性,$f(\boldsymbol x)&lt;0$时$y=-1$。且$\hat\gamma_i$越大该样本点分类的可信度越高。但是当成比例改变$\boldsymbol w$和$b$时超平面不变，但函数间隔会变大。用几何间隔能更好的解决这个用问题，即样本点到超平面的距离：\begin{align}\gamma_i=\cfrac{|\boldsymbol w^{\mathrm{T}}\boldsymbol x+b|}{||\boldsymbol w||}=y_i\cfrac{\boldsymbol w^{\mathrm{T}}\boldsymbol x+b}{||\boldsymbol w||}=\cfrac{\hat\gamma_i}{||\boldsymbol w||}\end{align}对于一整个样本集我们把最小的样本点的间隔当做整个样本的间隔即：\begin{align}\gamma=\min\limits_{i=1,\cdots,n} \hat\gamma_i\end{align} 函数间隔我觉得有点鸡肋，不是很明白其存在的意义，直接用几何上的直观解释，点到超平面距离即几何间隔就可以解决的问题。 任意点到超平面距离公式证明。设$\boldsymbol x_0$是样本点在超平面上的投影，即$\boldsymbol w^\mathrm{T}\boldsymbol x_0+b=0$，则$\overrightarrow{\boldsymbol x_0\boldsymbol x_i}$与法向量$\boldsymbol w$平行，所以：\begin{align}|\boldsymbol w\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=|\boldsymbol w||\overrightarrow{\boldsymbol x_0\boldsymbol x_i}|=\sqrt{ w_1^2+\cdots+ w_n^2}d=||\boldsymbol w||d\end{align}又因为$\boldsymbol w^{\mathrm T}\boldsymbol x_0=-b$\begin{align}\boldsymbol w\cdot\overrightarrow{\boldsymbol x_0\boldsymbol x_i}&amp;= w^1(x_i^1-x_0^1)+\cdots+ w^n(x_i^n-x_0^n)\\&amp;= w^1x_i^1+\cdots+ w^nx_i^n-( w^1x_0^1+\dots+ w^ix_0^i)\\&amp;=\boldsymbol w^{\mathrm T}\boldsymbol x_i+b\end{align}则有\begin{align}|\boldsymbol w^{\mathrm T}\boldsymbol x_i+b|=||\boldsymbol w||d\\d=\cfrac{|\boldsymbol w^{\mathrm T}\boldsymbol x_i+b|}{||\boldsymbol w||}\end{align} 最大化间隔假设使样本中离超平面最近的样本点与超平面保证一定距离（可以证明，这样的超平面只有一个），令最近的样本点的函数间隔为$y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)$=1，这些最近的样本点被称为支持向量。则支持向量到超平面的几何间隔为：\begin{align}\gamma=\cfrac{1}{||\boldsymbol w||}\end{align}所以为了最大化间隔，目标函数为：\begin{align}\max\limits_{\boldsymbol w,b}&amp;\quad\cfrac{2}{||\boldsymbol w||}\\s.t.&amp;\quad y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)\geq1,\quad i=1,2,\cdots,m.\nonumber\end{align}为了求解方便目标函数（转化为凸优化问题），可将上式改为：$$\begin{align}\min\limits_{\boldsymbol w,b}&amp;\quad\cfrac{1}{2}||\boldsymbol w||^2\\s.t.&amp;\quad y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)\geq1,\quad i=1,2,\cdots,m.\nonumber\end{align}$$ 优化求解（对偶问题）求解凸优化问题可以用常用的二次规划优化包求解，也可以更有效的用拉格朗日乘数法求其对偶问题。 这样做的优点在于：一是对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。则该式的拉格朗日函数为(注意这里把约束条件变成了$g_i(x)\leq0$的形式)：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha)=\cfrac{1}{2}||\boldsymbol w||^2+\sum_{i=1}^{m}{\alpha_i(1-y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b))}\end{align}根据拉格朗日对偶性，构造函数：\begin{align}\theta_P(\boldsymbol w,b)=\max\limits_\alpha L(\boldsymbol w,b,\boldsymbol\alpha)=\cfrac{1}{2}||\boldsymbol w||^2\end{align}则原问题为:\begin{align}\min\limits_{\boldsymbol w,b}\cfrac{1}{2}||\boldsymbol w||^2=\min\limits_{\boldsymbol w,b}\max\limits_\alpha L(\boldsymbol w,b,\boldsymbol\alpha)\end{align}所以交换$min$,$max$得到其对偶问题为:\begin{align}\max\limits_\alpha \min\limits_{\boldsymbol w,b}L(\boldsymbol w,b,\boldsymbol\alpha)\end{align}对$L(\boldsymbol w,b,\boldsymbol\alpha)$求偏导并令其值为零。对$\boldsymbol w$，$b$有：\begin{align}&amp;\boldsymbol w+\sum_{i=1}^{m}{\alpha_i(0-\boldsymbol x_iy_i)}=0\\&amp;0+\sum_{i=1}^{m}{\alpha_i(0-y_i)}=0\end{align}化简得到：\begin{align}&amp;\boldsymbol w=\sum_{i=1}^{m}{\alpha_i\boldsymbol x_iy_i}\\&amp;0=\sum_{i=1}^{m}{\alpha_iy_i}\end{align}利用上述两式可以得到：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha)=&amp;\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\\&amp;\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_i\left(\left(\sum_{i=1}^m\alpha_jy_j\boldsymbol x_j\right)\cdot\boldsymbol x_i+b\right)\\&amp;=-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\end{align}因为在$\boldsymbol w$,$b$倒数为零时，$L(\boldsymbol w,b,\boldsymbol\alpha)$取得极小值，所以在约束条件$\sum_{i=1}^{m}{\alpha_iy_i}=0$下:\begin{align}\min\limits_{\boldsymbol w,b}L(\boldsymbol w,b,\boldsymbol\alpha)=-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\end{align}此时对偶问题可以写为:\begin{align}&amp;\max\limits_{\alpha}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.\quad &amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,\cdots,m\end{array}\end{align}也可以写成:\begin{align}&amp;\min\limits_{\alpha}\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}-\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.\quad &amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,\cdots,m\end{array}\end{align}关于这个式子的求解在下一篇(大概)，SMO算法中写上。下面看看为什么在边界的点叫做支持向量：KKT条件成立的式子有:\begin{align}\cfrac{\partial L(\boldsymbol w^{*},\alpha^{*},\beta^{*})}{\boldsymbol w}=0\\end{align}所以:\begin{align}\boldsymbol w^{*}=\sum_{i=1}^{m}{\alpha_i^{*}\boldsymbol x_iy_i}\end{align}则超平面可以写成:\begin{align}(\boldsymbol w^{*})^{\mathrm T}\boldsymbol x+b^{*}=\left(\sum_{i=1}^{m}{\alpha_i^{*}\boldsymbol x_iy_i}\right)^{\mathrm T}\boldsymbol x+b^{*}=\sum_{i=1}^m{\alpha_i^{*}y_i\boldsymbol x_i^{\mathrm T}\boldsymbol x}+b^{*}\end{align}从分离超平面的公式可以看到在计算时，只需要计算训练数据中对应于$\alpha_i^{*}\neq0$的点，而这些点满足$y_i(\boldsymbol w^\mathrm{T}\boldsymbol x_i+b)-1=0$,即只有支持向量对SVM分离超平面的计算有用。SVM对于线性可分的数据是完美的，对于线性不可分问题(噪声，特异点)我们有Soft SVM（下节）。关于拉格朗日对偶一般优化问题可以写成以下形式:\begin{align}&amp;\min\limits_{x}f(x)\\&amp;s.t.\begin{cases}g_i(x)\leq0\\h_i(x)=0\end{cases}\end{align}根据拉格朗日方法，对应的拉格朗日函数则为：\begin{align}L(x,\alpha,\beta)=f(x)+\sum_i{\alpha_i}g_i(x)+\sum_i{\beta_ih_i(x)}\end{align}其中$\alpha$,$\beta$都是拉格朗日乘数，且要求$\alpha_i\geq0$，$\beta$的值任意。构造函数:\begin{align}\theta_P(x)=\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\end{align}由于$h_i(x)$为零，所以最后一项为零,又$g_i(x)\leq0$且$\alpha_i\geq0$,所以$L(x,\alpha,\beta)$最大值时,$\alpha=0$，此时$L(x,\alpha,\beta)=f(x)$,即$\theta_P(x)$等价于满足约束条件情况的的$f(x)$,$\theta_P(x)$可以表达为下式：\begin{array}\theta_P(x)=\begin{cases}f(x)\quad&amp;\text {$g_i(x)\leq0$&amp;$h_i(x)=0$}\\\+\infty\quad &amp;\text{$g_i(x)&gt;0$&amp;$h_i(x)\neq0$}\end{cases}\end{array}可以看到$\theta_P(x)$对原约束条件进行了吸收，使原来的约束优化问题变成了无约束优化问题，所以最初的优化问题可以写为:\begin{align}\min\limits_x\theta_P(x)=\min\limits_x\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\end{align}P代表primal，上式中的问题被称为原问题,和最开始的约束优化问题等价,将$min$和$max$交换顺序就变成了原问题的对偶问题,D代表dual：\begin{align}\max\limits_{\alpha,\beta:\alpha_i\geq0}\theta_D(\alpha,\beta)=\max\limits_{\alpha,\beta:\alpha_i\geq0}\min\limits_xL(x,\alpha,\beta)\end{align}注意这里自变量变成了$\alpha,\beta$。记$p^{*}$为原问题的最优解，对应最优解的最优变量取值为$x^{*}$。$d^{*}$为对偶问题的最优解，对应最优解的最优变量为$\alpha^{*}$，$\beta^{*}$。则有$d^{*}\leq p^{*}$。下面证明为什么有$d^{*}\leq p^{*}$,对于任意$\alpha$,$\beta$,$x$有\begin{align}\theta_D(\alpha,\beta)&amp;=\min\limits_xL(x,\alpha,\beta) \\&amp;\leq L(x,\alpha,\beta)\\&amp;\leq\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\\&amp;=\theta_P(\alpha,\beta)\\\end{align}又原问题和对偶问题都有最优解，所以：\begin{align}d^{*}&amp;=\max\limits_{\alpha,\beta:\alpha_i\geq0}\min\limits_xL(x,\alpha,\beta)\\&amp;=\max\limits_{\alpha,\beta:\alpha_i\geq0}\theta_D(\alpha,\beta)\leq\min\limits_x\theta_P(x)\\&amp;=\min\limits_x\max\limits_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)\\&amp;=p^{*}\end{align}Slater条件:指严格满足不等式约束条件,即$g(x){&lt;}0,(i=1,…,m)$,对于SVM问题，亦即要求“数据是可分的”,当数据不可分时,强对偶（Strong Duality）不成立。Strong Duality:原问题是Convex的，并且满足Slater条件，此时$\min\max L≥\max\min L$的等号成立。即$p^{*}=d^{*}$,这里需要注意一下，QP是凸优化问题的一种特殊情况。当“Strong Duality”关系成立时，一定存在$x^{*},\alpha^{*},\beta^{*}$,使得$x^{*}$是原问题的解，$\alpha^{*},\beta^{*}$是对偶问题的解。记primal, dual分别为原问题和对偶问题,那么有$p^{*}=d^{*}=L(x^{*}$,$\alpha^{*}\beta^{*}$成立，并且$x^{*},\alpha^{*},\beta^{*}$满足KKT条件(充分必要条件):\begin{align}\begin{array}\\\{\alpha_i}^{*}\geq0 \quad &amp;i=1,…,m\\\g_i(x^{*})\leq0 \quad &amp;i=1,…,m\\\{\alpha_i}^{*}g_i(x^{*})=0\quad &amp;i=1,…,m\\ \cfrac{\partial L(x^{*},\alpha^{*},\beta^{*})}{x_i}=0\quad &amp;i=1,…,m&amp;\\\\cfrac{\partial L(x^{*},\alpha^{*},\beta^{*})}{\beta_i}=h_i(x)=0\quad &amp;i=1,…,m&amp;\end{array}\end{align}其中式$\alpha_i^{*}g_i(x^{*})=0$被称为对偶互补条件,由此式可以知道，若$\alpha_i^{*}&gt;0$,则有$g_i(x^{*})=0$。对于SVM来说,这意味着只有离超平面最近的边界点（$g_i(x)=0$）其系数 $\alpha_i&gt;0$,对于其他的点都有$\alpha_i=0$。 参考文献https://www.cnblogs.com/dreamvibe/p/4349886.htmlhttps://blog.csdn.net/diligent_321/article/details/53396682]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>笔记</tag>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之SVM2(支持向量机)]]></title>
    <url>%2F2018%2F04%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8BSVM2(%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)%2F</url>
    <content type="text"><![CDATA[这一节写软间隔最大化支持向量机和核技巧。 Soft SVMSVM有时候会遇到数据线性不可分的问题,线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件,为了解决这个问题，我们使用Soft SVM,即在原来的超平面方程中加入松弛变量$\xi\geq0$允许某些样本可以在原约束条件下犯错,使函数间隔加上这个变量后大于等于1,即约束条件变为:\begin{align}y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b)\geq1-\xi_i\\\xi\geq0,i=1,…,m\end{align}样本中每一个变量对应一个松弛变量，此时目标函数变为(对于每一个松弛变量$\xi_i$,支付一个代价$\xi_i$):\begin{align}L(\boldsymbol w,\xi)=\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\end{align}这里的$C$是惩罚参数,$C$越大对于误分类的惩罚越大。最小化上式则表示在最大化间隔同时使误分类的样本数目尽可能的少。 Soft SVM的原始问题可以写作:\begin{align}&amp;\min\limits_{\boldsymbol w,b,\boldsymbol\xi_i}\quad\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\\&amp;\begin{array}\\s.t.\quad &amp;y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b)\geq1-\xi_i\\&amp;\xi\geq0,i=1,…,m\end{array}\end{align}上式依旧是一个凸二次规划问题，按照拉格朗日乘数法，得到拉格朗日函数为：\begin{align}L(\boldsymbol w,b,\boldsymbol\alpha,\boldsymbol\xi,\boldsymbol\mu)=&amp;\cfrac{1}{2}||\boldsymbol w||^2+C\sum_{i=1}^m{\xi_i}\\&amp;+\sum_{i=1}^m{\alpha_i(1-\xi_i-y_i(\boldsymbol w^{\mathrm T}\boldsymbol x_i+b))}-\sum_{i=1}^m\mu_i\xi_i\end{align}$\alpha_i$,$\mu_i$均为拉格朗日系数,和上节一样可以得到对偶问题为:\begin{align}\max\limits_{\alpha_i,\mu_i}\min\limits_{\boldsymbol w,b,\boldsymbol\xi}L(\boldsymbol w,b,\boldsymbol\alpha,\boldsymbol\xi,\boldsymbol\mu)\end{align} 求拉格朗日函数的偏导有:\begin{align}\boldsymbol w&amp;=\sum_{i=1}^m\alpha_iy_i\boldsymbol x_i\\0&amp;=\sum_{i=1}^m\alpha_iy_i\\C&amp;=\alpha_i+\mu_i\end{align}把上述式子带入对偶问题的函数化简:\begin{align}&amp;\max\limits_{\alpha_i}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j(\boldsymbol x_j\cdot\boldsymbol x_i)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.&amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,…,m\\&amp;0\leq\alpha_i\leq C\\\end{array}\end{align}同样类似于硬间隔SVM，软间隔支持向量机也满足KKT条件,则从上式可以得到, 若$\alpha_i=0$,则这些样本对模型不会产生影响。 若$0&lt;\alpha_i&lt; C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i&gt;0$,从而有$\xi_i=0$即这些点位于最大间隔边界上,就是我们的支持向量。 若$\alpha_i=C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i=0$,此时若$0&lt;\xi_i&lt;1$,则分类正确，样本点在间隔边界和分离超平面之间。若$\xi_i=1$，则分类正确，样本点在分离超平面上。 若$\alpha_i=C$,则有$y_if(\boldsymbol x_i)-1+\xi_i=0$,$\mu_i=0$,此时若$\xi_i&gt;1$,则分类错误，样本点位于分类超平面另一边。所以最大间隔边界上的样本点可能是支持向量，也有可能是离群点。软间隔支持向量机的最终模型仍然仅与支持向量有关，保持了稀疏性。 损失函数常见的代替损失函数 hinge损失(hinge loss):$l_{hinge}(z)=max(0,1-z)$ 指数损失(exponential loss):$l_{exp}(z)$=exp(-z) 对率损失(logistic loss):$l_{log}=log(1+exp(-z))$Liner SVM用hinge损失。 核函数有时候数据在原始样本空间线性不可分,但可以将样本从原始空间映射到一个更高维的特征空间(希尔伯特空间),使样本在这个空间内线性可分。而且如果原始空间是有限维，即属性有限，那么一定存在一个高维特征空间使样本线性可分。把原始空降映射到高维空间叫做kernel trick核技巧，同时不会增加计算量。 我们假设$\phi(\boldsymbol x)$为讲原始空间的样本$\boldsymbol x$映射后的特征向量。 与之前推导一样我们可以得到对偶问题为:\begin{align}&amp;\max\limits_{\alpha_i}-\cfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m{\alpha_iy_i\alpha_jy_j\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)}+\sum_{i=1}^m\alpha_i\\&amp;\begin{array}\\s.t.&amp;\sum_{i=1}^{m}{\alpha_iy_i}=0\\&amp;\alpha_i\geq0,i=1,2,…,m\\\end{array}\end{align}在高维空间计算$\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)$通常比较困难,设计这样一个函数:\begin{align}\kappa(\boldsymbol x_i,\boldsymbol x_j)=\phi{(\boldsymbol x_i)}^{\mathrm T}\phi(\boldsymbol x_j)\end{align}即$\boldsymbol x_i$与$\boldsymbol x_j$在高维空间的内积等于他们在原始样本空间通过函数$\kappa(\boldsymbol x_i,\boldsymbol x_j)$计算的结果，这个技巧叫核技巧，这个函数就是核函数。 核技巧的思想是不显示的表示映射函数,而是直接计算核函数。这样最终求解超平面方程为:\begin{align}f(\boldsymbol x)=\sum\limits_{i=1}^m{\alpha_iy_i\kappa(\boldsymbol x_,\boldsymbol x_i)}+b\end{align}如何找到核函数？定理:只要一个堆成函数所对应的和核矩阵半正定，他就能作为核函数使用。 常用核函数 线性核 $\kappa(\boldsymbol x_i,\boldsymbol x_j)=\boldsymbol x_i^{\mathrm T}\boldsymbol x_j$ 多项式核 $\kappa(\boldsymbol x_i,\boldsymbol x_j)=(\boldsymbol x_i^{\mathrm T}\boldsymbol x_j)^d\quad d\geq1$为多项式次数 高斯核$\kappa(\boldsymbol x_i,\boldsymbol x_j)=exp\left(-\cfrac{||\boldsymbol x_i-\boldsymbol x_j||^2}{2\sigma^2}\right)\quad\sigma&gt;0$为高斯核带宽 参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>笔记</tag>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
        <tag>soft SVM</tag>
        <tag>核技巧</tag>
      </tags>
  </entry>
</search>
